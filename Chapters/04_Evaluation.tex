\chapter{Evaluation}
%

\section{Introduction}
%In image processing field and computer vision, segmentation consists in partitioning an image by means of the differences between an object of interest and the background. It is still considered a challenging task that depends on the image properties and its specific context. Vessel segmentation is mainly composed by the extraction of the vascular lumen which conserves a tubular shape (linear in 2D images) in normal cases and its an important step for posterior visualization techniques and pathology quantification. However, numerous complications appear caused by changes in vessels sizes and curvatures, inter- and intra-patient variability, proximity to similar surrounding structures, motion and acquisition artifacts, and presence of pathologies. Due to these difficulties, simple segmentation methods does not obtain an explicit and generic solution that covers the most part of the cases.

%Many recent advanced studies has focused on this problematic, specially in coronary arteries~\citep{Lesage2009Thesis, Schaap2010Thesis, Wang2011Thesis, Zuluaga2011Thesis}. Additionally, image processing challenges and reviews try to establish a general framework to compare methods and results obtained by different research groups. While some surveys presented classifications focusing on the algorithmic details, which do not facilitate the comparisons~\citep{Kirbas2004}, a more recent survey in 3D vessel segmentation~\citep{Lesage2009b} proposed a synthetic categorization based on three different criteria: 1) \textit{a priori} knowledge about vessel geometry and appearance models, 2) definition and use of features that describe vascular lumen, and 3) schemes that have been proposed to perform lumen segmentation. The latter category can be roughly separated in two groups: 3a)algorithms requiring a previously extracted centerline (axis) and 3b) methods running in one pass~\citep{Schaap2011}.

%This chapter will be mainly focuses on the most recent works in coronaries, specially the ones falling in the 3a group. Various axis extraction methods have been evaluated in~\citep{Metz2008} and demonstrated a high accuracy compared to reference axes. For this reason, even if it is still a hard task in terms of image processing, the results confirm the current feasibility of the assumption and the availability of a correct centerline for posterior processing tasks. In most of the cases, centerlines are also needed for posterior processing task such as stenosis detection and quantification.

\section{Materials}
 
The datasets were obtained from the publicly available Rotterdam Coronary Artery Algorithm Evaluation Framework (http://coronary.bigr.nl/). A total of twenty-six multi-center multi-vendor datasets were used for this study. Eight datasets from centerline extraction challenge, and the rest of them from the stenoses challenge. 

METER TABLA DE RESUMEN DE DATASETS DE CAT

For additional information about the image acquisition, data selection and reference standards, the reader may refer to ~\citep{Schaap2009} and KirisÌ§li et al. ~\citep{Kirisli2013} articles, or to the framework website.

\section{Evaluation Measures}

Four different measures (three for overlapping and one for accuraccy assesment) are used in this section. All of them are based on the terms defined by \citep{Schaap2009} (See \ref{fig:exp3_ms}).

\begin{figure}[htbp]
	\centering
		\includegraphics[width=1.0\textwidth]{./Figures/eval_ms.png}
	\caption[Coronary Centerline Evaluation Measures CAT08]{Evaluation Measures used at CAT08 (source: \citep{Schaap2009}).}
	\label{fig:exp3_ms}
\end{figure}

From the overlap point of view, overlap \textbf{(OV)} represents the ability to track the complete vessel annotated by the human observers; overlap until first error \textbf{(OF)} determines how much of a coronary artery has been extracted before making the first error; Finally, overlap with the clinically relevant part of the vessel \textbf{(OT)} gives an indication how the method is able to track a clinical relevant vessel section. An algorithm score of 100 points is a perfect match between us and the reference, a score equal to 0 is a total failure in the extraction. In the accuraccy side, average inside \textbf{(AI)} is the average distance inside the vessel, it uses the annotated radius to score it. The observer score equal to 50 points, say if we do better ($>$50 points) or worst ($<$50 points).

\section{Centerline Extraction Experiments}

Both quantitative and qualitative evaluations were conducted to assess the performance of the method. The quantitative evaluation provides an objective evaluation of the method and enables comparison with previously published methods. However, comparison with results in literature should always be done with care, as results have been obtained on different data sets. A limitation of quantitative evaluation is that it requires a set of manually annotated structures, which is time consuming and not feasible for large numbers of 3D data. Therefore, we also conducted qualitative evaluations, as they can be performed in less time and hence on a larger number of data. The quantitative and qualitative evaluation measures used in this work were introduced in the section above.

\subsection{Experiment I: MFlux in CAT08 Challenge}

The centerline experiments where done using the eight training datasets from CAT08 challenge. For each patient, four vessels are provided in separated files. Since the consensus centerlines (done by three observers) are available in this experiment with their inter-observer variability measures, a quantitative analysis was performed using the standard framework proposed by ~\citep{Schaap2009}. Also, dataset results have been sent to the Web-based evaluation framework in order to verify the method results.

\subsection{Experiment II: MFlux in MICCAI2012 Challenge}

During the ``Stenoses Detection, Quantification and Lumen Segmentation challenge'', our team failed extracting some  vessels in problematic images (dataset 01 and dataset 05). Therefore, a correct and complete extraction is mandatory with this new version. Figure \ref{fig:exp2_dt05} shows our new framework processing one of the problematic datasets (dataset 05).

\begin{figure}[htbp]
	\centering
		\includegraphics[width=4.0in]{./Figures/dt05v01.png}
		\includegraphics[width=4.0in]{./Figures/dt05v01_cpr.png}
	\caption[MICCAI2012 Extraction]{\textit{Top}, Interactive RCA vessel extraction (dataset 05) . \textit{Bottom}, CPR visualization of the vessel extracted.}
	\label{fig:exp2_dt05}
\end{figure}

\subsection{Experiment III: MFlux VS Gulsun \& Tek (itkRGC)}

In the third experiment, we investigate the robustness of our method against a new version of Gulsun \& Tek developed by our team at Creatis. The performance of the team implementation (Gulsun \& Tek) was evaluated on the same CAT08 datasets. An additional results submission was done to the Web-platform. Also, problematic datasets at MICCAI2012 were visually evaluated with the two implementations (MFlux, G \& T). Figure ~\ref{fig:exp3_cpr} shows an axe extracted by the methods against the reference.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=4.0in]{./Figures/cpr_lkeb.png}
		\includegraphics[width=4.0in]{./Figures/cpr_gt.png}
		\includegraphics[width=4.0in]{./Figures/cpr_mflux.png}
	\caption[Centerline Comparison]{Centerline visual assesment between: \textit{Top}, MICCAI 2012 Reference. \textit{Middle}, G \& T axe. \textit{Bottom}, MFlux axe.}
	\label{fig:exp3_cpr}
\end{figure}

\section{Supervised Learning Experiments in Lesion Detection}

\subsection{Experiment I: RUSBoost in vessel hyphotesis}

\subsection{Experiment II: RUSBoost in segment hyphotesis}

\subsection{Experiment III: RUSBoost VS Random Forest}

